---
title: "Homework 3"
author: "Mufeng Xu"
date: '`r format(Sys.time(), "%Y-%m-%d")`'
output: github_document

---

```{r setup, include=FALSE}
library(tidyverse)
library(p8105.datasets)
library(patchwork)
knitr::opts_chunk$set(
	fig.width = 6, 
  fig.asp = .6,
  out.width = "90%"
)
theme_set(theme_minimal() + theme(legend.position = "bottom"))
options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)
scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```


### Problem 1

```{r}
data("instacart")
```

This dataset contains `r nrow(instacart)` rows and `r ncol(instacart)` columns. 

Observations are the level of items in orders by user. There are user / order variables -- user ID, order ID, order day, and order hour. There are also item variables -- name, aisle, department, and some numeric codes. 

How many aisles, and which are most items from?

```{r}
instacart %>% 
	count(aisle) %>% 
	arrange(desc(n))
```


Let's make a plot

```{r}
instacart %>% 
	count(aisle) %>% # number the items ordered in each aisle
	filter(n > 10000) %>% # limiting aisles with >10000 items
	mutate(
		aisle = factor(aisle),
		aisle = fct_reorder(aisle, n)
	) %>% 
	ggplot(aes(x = aisle, y = n)) +  
	geom_point() + # scatterplot
	theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1)) # rotate the axies 90 degrees
```


Let's make a table!!

```{r}
instacart %>% 
	filter(aisle %in% c("baking ingredients", "dog food care", "packaged vegetables fruits")) %>% 
	group_by(aisle) %>% 
	count(product_name) %>% 
	mutate(rank = min_rank(desc(n))) %>% 
	filter(rank < 4) %>% 
	arrange(aisle, rank) %>% 
	knitr::kable()
```


Apples vs ice cream..

```{r}
instacart %>% 
	filter(product_name %in% c("Pink Lady Apples", "Coffee Ice Cream")) %>% 
	group_by(product_name, order_dow) %>% 
	summarize(mean_hour = mean(order_hour_of_day)) %>% 
	pivot_wider(
		names_from = order_dow,
		values_from = mean_hour
	) %>% 
  knitr::kable()
```


### Problem 2 

Load, Tidy and Wrangle the data
```{r}
accel_df = read.csv(
  "./data/accel_data.csv") %>% 
  janitor::clean_names() %>% 
  pivot_longer(
    activity_1:activity_1440,
    names_prefix = "activity_",
    names_to = "minute_of_the_day",
    values_to = "activity_count"
  )

# Create a "day" dataframe to arrange the dates in order
day_df = 
  tibble(
    day_num = 0:6,
    day = c("Sunday", "Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday")
  )

# Join the accel_df and day_df together
accel_tidy = 
  left_join(accel_df, day_df, by = "day") %>% 
  mutate(
    day = as.factor(day),
    minute_of_the_day = as.integer(minute_of_the_day)
  ) %>% 
  mutate(
    weekday_vs_weekend =
      case_when(day_num == 1:5 ~ "weekday", day_num == 0 ~ "weekend", day_num == 6 ~"weekend"),
    weekday_vs_weekend = as.factor(weekday_vs_weekend)
  ) %>% 
  relocate(week, day, day_num, weekday_vs_weekend, minute_of_the_day) %>% 
  arrange(week, day_num, minute_of_the_day)
accel_tidy
```

The study examines "the accelerometer data collected on a 63 year-old male with BMI 25, who was admitted to the Advanced Cardiac Care Center of Columbia University Medical Center and diagnosed with congestive heart failure(CHF)". The study duration is `r accel_tidy %>% distinct(week, day) %>% nrow` days, and `r accel_tidy %>% distinct(minute_of_the_day) %>% nrow` minutes per day. After tidying and wrangling the original data, the resulting dataset has `r nrow(accel_tidy)` rows(observations) and `r ncol(accel_tidy)` columns(variables), including variables of `r ls(accel_tidy)` with corresponding class of `r lapply(accel_tidy, class)`.

Find the total activity of each day

```{r}
accel_summary = 
  tibble(accel_tidy) %>% 
  group_by(week, day) %>% 
  summarize(activity_count_of_the_day = sum(activity_count)) %>% 
  pivot_wider(
    names_from = day,
    values_from = activity_count_of_the_day
  ) %>% 
  relocate('week', 'Sunday', 'Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday') %>% 
  mutate(
    week_average = mean(Sunday:Saturday)
  )

print(accel_summary)
```

From looking at the week averages for each week, it can be observed that the patient has similar number of activities on week 1 and week 2. As passing on to week 3, the number of activities decreases a bit. On week 4 and 5, number of activities significantly decreases. Week 2 > Week 1 > Week 3 >> Week 4 > Week 5.
For week 1-5, the average activities in a day are `r knitr::kable(accel_summary %>% select(Sunday:Saturday) %>% colMeans)` It shows that the patients have the least activities on Saturdays, and the most activities on Fridays. Friday > Wednesday > Thursday > Sunday > Monday > Tuesday > Saturday.


Make a plot
 
```{r}
accel_tidy %>% 
  ggplot(aes(x = minute_of_the_day, y = activity_count, color = day)) +
  geom_line(alpha = 0.7) +
  labs(
    title = "Activity over the Day",
    x = "Minutes of the Day (min)",
    y = "Number of Activities"
  ) +
  scale_x_continuous(
    breaks = seq(0, 1440, by = 60)
  ) +
  theme(axis.text.x = element_text(angle = 90))
```

Based on the single-panel plot, it can be observed that from 0am to 5am, the patient has a very small number of activities; from 5am (300min) and on, the patient starts to have a larger number of activities. On several days of the week, the patient has peak numbers of activities around: 7am, 10am to 12:30pm(Sundays); 9pm (Mondays); no obvious peak for Tuesdays; 6am, 7:30pm(Wednesday); 7am(Thursday); 9am and 8:30pm to 10pm (Fridays); 4:30pm, 8pm(Saturday).


## Problem 3

```{r}
data("ny_noaa")
```

The original dataset `ny_noaa` contains `r nrow(ny_noaa)` rows and `r ncol(ny_noaa)` columns, containing variables of `r ls(ny_noaa)`. It describes different weather parameters - precipitation(tenths of mm), snowfall(mm), snow patch(mm), maximum and minimum temperature (tenths of degree Celsius) - in different weather stations over years. From the data set, the table below shows the missing values of each variable.

```{r}
summary(ny_noaa)
```



 By using the summary function (output below), we can see there is data missing for all five of the weather-related variables.


Clean and Tidy the dataset
```{r}
ny_noaa = 
  tibble(ny_noaa) %>% 
  janitor::clean_names() %>% 
  mutate(
    year = lubridate::year(date),
    month = lubridate::month(date),
    day = lubridate::day(date)
  ) %>% # separate year, month, date
  mutate(
    prcp = prcp / 10,
    tmax = as.numeric(tmax) / 10,
    tmin = as.numeric(tmin) / 10
    ) %>% 
  filter(!is.na(tmax)) %>% 
  arrange(id, year, month, day) %>% 
  relocate(id, year, month, day)

ny_noaa %>% 
  count(snow) %>% 
  arrange(desc(n))
```

The most commonly observed values for snowfall is 0 because in general, there is only snowfalls in winters. The dataset records a zero on each and everyday there is no snowfalls.


Make Plots of the average max temperature in January and in July in each station across years


First thing to do is to do some munipulations to the dataset:
```{r}
ny_noaa_JJ = 
  tibble(ny_noaa) %>% 
  filter(month == "1" | month == "7") %>% 
  filter(!is.na(tmax)) %>% 
  select(-date, -prcp, -snow, -tmin, -snwd) %>% 
  group_by(id, month, year) %>% 
  summarize(
    tmax_average = mean(tmax, na.rm = TRUE)
      ) %>% 
  arrange(id, year, month) %>% 
  relocate(id, year, month)
```

Second, it's time to plot.

```{r}
ny_noaa_JJ %>% 
  ggplot(aes(x = year, y = tmax_average, group = id, color = month)) +
  geom_point(alpha = 0.3) +
  facet_grid(. ~ month) +
  scale_x_continuous(breaks = seq(1980, 2010, by = 1)) +
  labs(
    title = "Average Max Temperatures in January and July across Years",
    x = "Year",
    y = "Average Max Temperature (C)"
  ) +
  theme(axis.text.x = element_text(angle = 90))
```

Comparing the two scatterplots for January and July tmax averages in different stations across years, it can be observed that the average maximum temperatures in January is lower than that in July. The range of tmax in January is larger than that in July that in different stations, the tmax detected has larger variance in January. Maximum temperature in January is less stable than than in July that the major trends of scatter points in January are floating. Also, there are outliers shown in the graph. For example, in January, 1881, there is one station detecting tmax significantly lower than that of the other stations. 


Make another two-panel plots - patchworkï¼Œ violin

First is to make a tmax and tmin plot
```{r}
tmin_tmax = 
  ny_noaa %>% 
  filter(!is.na(tmax)) %>% 
  filter(!is.na(tmin)) %>% 
  ggplot(aes(x = tmin, y = tmax)) +
  geom_smooth() +
  labs(
    title = "Maximum Temperature vs. Minimum Temperature",
    x = "Min Temperature (C)",
    y = "Max Temperature (C)"
  ) +
  theme(legend.position = "none")
  
```

Second is to find the snowfall plot

```{r}
snowfall = 
  ny_noaa %>% 
  filter(snow > 0 ,snow < 100) %>% 
  ggplot(aes(x = year, y = snow, group = year)) +
  geom_boxplot() +
  theme(axis.text.x = element_text(angle = 90)) +
  scale_x_continuous(breaks = seq(1980, 2010, by = 1)) +
  labs(
    title = "Boxplot of Snowfall Values from 1981-2010",
    x = "Year",
    y = "Snowfall(mm)",
    caption = "For only snowfall values greater than 0 and less than 100mm"
  ) +
  theme(legend.position = "none")
```


Patchwork: join two plots together

```{r}
tmin_tmax / snowfall
```







